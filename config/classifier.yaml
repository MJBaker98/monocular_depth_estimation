head:
  mlp:
    n_hidden_layers: 3
    n_inputs: 9
    n_outputs: 10
    n_hidden_nodes: 32
    activation: 'ReLU'

vit:
  model_dim: 9
  embed:
    img_size: [32, 32]
    patch_size: 16
    in_channels: 1
    out_channels: 9
  transformer:
    n_heads: 3
    n_channels: 1
    tfm_depth: 3
    
  mlp:
    n_hidden_layers: 2
    n_hidden_nodes: 32
    activation: 'GELU'

train:
  n_epochs: 5
  batch_size: 512
  lr: 0.01

