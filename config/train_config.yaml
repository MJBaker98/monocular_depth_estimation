depth:
  mlp:
    n_hidden_layers: 3
    n_inputs: 32
    n_outputs: 1000  # image size flattened
    n_hidden_nodes: 32
    activation: 'ReLU'

vit:
  encode:
    seq_len: 256
    model_dim: 512
  embed:
    img_size: [256,256]
    patch_size: 16
    in_channels: 3
    out_channels: 7
  transformer:
    dim: 512
    n_heads: 5
    n_channels: 32
    tfm_depth: 5
    
  mlp:
    n_hidden_layers: 2
    n_hidden_nodes: 32
    activation: 'GELU'

